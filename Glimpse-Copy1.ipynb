{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 16\n",
    "batch_size = 13\n",
    "target_size = 17\n",
    "hidden_size = 8\n",
    "n_head = 2\n",
    "single_dim = hidden_size // n_head\n",
    "query = torch.FloatTensor(batch_size, input_size).uniform_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor(batch_size, target_size, input_size).uniform_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Linear(input_size, hidden_size, bias=False)\n",
    "W_k = nn.Linear(input_size, hidden_size, bias=False)\n",
    "W_v = nn.Linear(input_size, hidden_size, bias=False)\n",
    "W_out = nn.Linear(hidden_size, input_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_c = W_q(query)\n",
    "k = W_k(target)\n",
    "v = W_v(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 8]), torch.Size([13, 17, 8]), torch.Size([13, 17, 8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_c.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_r = v.reshape(batch_size, target_size, n_head, single_dim).permute(0, 2, 1, 3) \n",
    "#batch_size x n_head x target_size x single_head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 2, 17])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret1 = torch.einsum(\"ik,ijk->ijk\", [q_c,k])\n",
    "ret1 = ret1.reshape(batch_size, target_size, n_head, single_dim).sum(-1) / sqrt(single_dim)\n",
    "ret1 = ret1.permute(0, 2, 1) #batch_size x n_head x query_size x target_size\n",
    "ret1.shape\n",
    "r_softmax = ret1.softmax(-1)\n",
    "r_softmax.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0595, 0.0577, 0.0603, 0.0567, 0.0598, 0.0578, 0.0573, 0.0600,\n",
       "          0.0598, 0.0574, 0.0585, 0.0587, 0.0599, 0.0585, 0.0605, 0.0583,\n",
       "          0.0594],\n",
       "         [0.0543, 0.0621, 0.0597, 0.0586, 0.0613, 0.0554, 0.0556, 0.0580,\n",
       "          0.0556, 0.0488, 0.0582, 0.0569, 0.0597, 0.0640, 0.0610, 0.0612,\n",
       "          0.0696]],\n",
       "\n",
       "        [[0.0562, 0.0588, 0.0631, 0.0614, 0.0546, 0.0552, 0.0613, 0.0534,\n",
       "          0.0592, 0.0614, 0.0605, 0.0620, 0.0643, 0.0558, 0.0540, 0.0610,\n",
       "          0.0577],\n",
       "         [0.0557, 0.0615, 0.0602, 0.0613, 0.0550, 0.0577, 0.0595, 0.0560,\n",
       "          0.0549, 0.0621, 0.0601, 0.0585, 0.0592, 0.0588, 0.0613, 0.0583,\n",
       "          0.0600]],\n",
       "\n",
       "        [[0.0579, 0.0586, 0.0588, 0.0583, 0.0589, 0.0591, 0.0592, 0.0577,\n",
       "          0.0584, 0.0600, 0.0592, 0.0586, 0.0597, 0.0595, 0.0582, 0.0602,\n",
       "          0.0579],\n",
       "         [0.0612, 0.0594, 0.0597, 0.0578, 0.0576, 0.0604, 0.0593, 0.0561,\n",
       "          0.0568, 0.0580, 0.0596, 0.0561, 0.0582, 0.0581, 0.0620, 0.0605,\n",
       "          0.0592]],\n",
       "\n",
       "        [[0.0582, 0.0592, 0.0580, 0.0578, 0.0597, 0.0575, 0.0598, 0.0588,\n",
       "          0.0595, 0.0595, 0.0574, 0.0600, 0.0591, 0.0592, 0.0597, 0.0577,\n",
       "          0.0589],\n",
       "         [0.0544, 0.0609, 0.0557, 0.0595, 0.0588, 0.0591, 0.0634, 0.0579,\n",
       "          0.0595, 0.0547, 0.0654, 0.0618, 0.0579, 0.0572, 0.0561, 0.0580,\n",
       "          0.0598]],\n",
       "\n",
       "        [[0.0583, 0.0553, 0.0612, 0.0532, 0.0578, 0.0539, 0.0557, 0.0566,\n",
       "          0.0630, 0.0574, 0.0613, 0.0608, 0.0613, 0.0646, 0.0618, 0.0552,\n",
       "          0.0625],\n",
       "         [0.0587, 0.0565, 0.0608, 0.0586, 0.0553, 0.0691, 0.0564, 0.0569,\n",
       "          0.0578, 0.0602, 0.0607, 0.0618, 0.0629, 0.0577, 0.0566, 0.0557,\n",
       "          0.0544]],\n",
       "\n",
       "        [[0.0599, 0.0597, 0.0585, 0.0591, 0.0559, 0.0583, 0.0607, 0.0598,\n",
       "          0.0578, 0.0615, 0.0619, 0.0551, 0.0573, 0.0596, 0.0621, 0.0560,\n",
       "          0.0568],\n",
       "         [0.0586, 0.0656, 0.0556, 0.0633, 0.0606, 0.0593, 0.0608, 0.0567,\n",
       "          0.0567, 0.0552, 0.0546, 0.0614, 0.0622, 0.0574, 0.0540, 0.0581,\n",
       "          0.0601]],\n",
       "\n",
       "        [[0.0615, 0.0619, 0.0591, 0.0553, 0.0610, 0.0601, 0.0541, 0.0532,\n",
       "          0.0601, 0.0523, 0.0544, 0.0620, 0.0610, 0.0602, 0.0603, 0.0672,\n",
       "          0.0563],\n",
       "         [0.0606, 0.0579, 0.0604, 0.0593, 0.0663, 0.0600, 0.0492, 0.0550,\n",
       "          0.0619, 0.0548, 0.0566, 0.0693, 0.0551, 0.0639, 0.0564, 0.0586,\n",
       "          0.0548]],\n",
       "\n",
       "        [[0.0583, 0.0582, 0.0584, 0.0569, 0.0603, 0.0598, 0.0586, 0.0603,\n",
       "          0.0592, 0.0582, 0.0625, 0.0536, 0.0589, 0.0602, 0.0568, 0.0607,\n",
       "          0.0591],\n",
       "         [0.0545, 0.0611, 0.0619, 0.0637, 0.0610, 0.0545, 0.0660, 0.0526,\n",
       "          0.0555, 0.0607, 0.0565, 0.0589, 0.0589, 0.0537, 0.0623, 0.0621,\n",
       "          0.0561]],\n",
       "\n",
       "        [[0.0588, 0.0590, 0.0579, 0.0590, 0.0603, 0.0599, 0.0596, 0.0576,\n",
       "          0.0604, 0.0591, 0.0576, 0.0591, 0.0593, 0.0585, 0.0585, 0.0579,\n",
       "          0.0578],\n",
       "         [0.0643, 0.0604, 0.0607, 0.0615, 0.0632, 0.0579, 0.0574, 0.0578,\n",
       "          0.0533, 0.0608, 0.0599, 0.0568, 0.0508, 0.0595, 0.0592, 0.0582,\n",
       "          0.0583]],\n",
       "\n",
       "        [[0.0558, 0.0581, 0.0594, 0.0605, 0.0608, 0.0551, 0.0602, 0.0590,\n",
       "          0.0600, 0.0580, 0.0576, 0.0594, 0.0596, 0.0563, 0.0607, 0.0586,\n",
       "          0.0609],\n",
       "         [0.0616, 0.0611, 0.0587, 0.0594, 0.0543, 0.0566, 0.0556, 0.0638,\n",
       "          0.0601, 0.0597, 0.0581, 0.0594, 0.0583, 0.0576, 0.0611, 0.0546,\n",
       "          0.0600]],\n",
       "\n",
       "        [[0.0582, 0.0624, 0.0589, 0.0571, 0.0564, 0.0614, 0.0587, 0.0590,\n",
       "          0.0641, 0.0610, 0.0590, 0.0582, 0.0561, 0.0586, 0.0552, 0.0566,\n",
       "          0.0590],\n",
       "         [0.0572, 0.0543, 0.0552, 0.0629, 0.0615, 0.0597, 0.0599, 0.0539,\n",
       "          0.0589, 0.0615, 0.0593, 0.0603, 0.0603, 0.0580, 0.0565, 0.0634,\n",
       "          0.0572]],\n",
       "\n",
       "        [[0.0615, 0.0624, 0.0612, 0.0574, 0.0589, 0.0561, 0.0634, 0.0594,\n",
       "          0.0590, 0.0555, 0.0547, 0.0590, 0.0571, 0.0581, 0.0620, 0.0579,\n",
       "          0.0564],\n",
       "         [0.0570, 0.0583, 0.0570, 0.0590, 0.0626, 0.0632, 0.0578, 0.0585,\n",
       "          0.0545, 0.0643, 0.0592, 0.0573, 0.0586, 0.0595, 0.0537, 0.0579,\n",
       "          0.0617]],\n",
       "\n",
       "        [[0.0598, 0.0584, 0.0589, 0.0557, 0.0583, 0.0621, 0.0545, 0.0582,\n",
       "          0.0586, 0.0582, 0.0588, 0.0558, 0.0546, 0.0613, 0.0616, 0.0642,\n",
       "          0.0610],\n",
       "         [0.0568, 0.0601, 0.0630, 0.0532, 0.0526, 0.0548, 0.0504, 0.0571,\n",
       "          0.0647, 0.0613, 0.0604, 0.0573, 0.0583, 0.0605, 0.0597, 0.0668,\n",
       "          0.0630]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 2, 4])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_r.shape, r_softmax.shape\n",
    "_ret = torch.matmul(v_r.transpose(-1, -2), r_softmax.unsqueeze(-1)).squeeze(-1)\n",
    "_ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ret = _ret.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 8])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = W_out(_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 3, 16])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_res.permute(0, 2, 1, 3).sum(-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0731e-01,  5.8213e-01,  3.3198e-01,  ...,  1.5870e-01,\n",
       "            2.5539e-01,  3.3690e-01],\n",
       "          [ 1.0836e-01,  5.8805e-01,  3.3941e-01,  ...,  1.5994e-01,\n",
       "            2.6216e-01,  3.3867e-01],\n",
       "          [ 1.0827e-01,  5.8202e-01,  3.3503e-01,  ...,  1.5979e-01,\n",
       "            2.5627e-01,  3.3735e-01]],\n",
       "\n",
       "         [[-1.1920e-01, -3.8302e-02,  2.5744e-01,  ..., -5.7536e-02,\n",
       "           -1.4551e-01,  1.4655e-01],\n",
       "          [-1.1925e-01, -3.8628e-02,  2.5704e-01,  ..., -5.7621e-02,\n",
       "           -1.4569e-01,  1.4673e-01],\n",
       "          [-1.1975e-01, -3.9917e-02,  2.5667e-01,  ..., -5.8352e-02,\n",
       "           -1.4713e-01,  1.4751e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 6.7252e-02,  6.6533e-01,  3.4646e-01,  ...,  1.6072e-01,\n",
       "            1.8352e-01,  4.2381e-01],\n",
       "          [ 6.7363e-02,  6.6733e-01,  3.4545e-01,  ...,  1.6095e-01,\n",
       "            1.8326e-01,  4.2508e-01],\n",
       "          [ 6.7806e-02,  6.6182e-01,  3.4423e-01,  ...,  1.6016e-01,\n",
       "            1.8385e-01,  4.2103e-01]],\n",
       "\n",
       "         [[-1.7025e-01, -6.0579e-02,  3.4141e-01,  ..., -1.3973e-02,\n",
       "           -1.7965e-01,  2.4099e-01],\n",
       "          [-1.7000e-01, -6.3900e-02,  3.3782e-01,  ..., -1.7574e-02,\n",
       "           -1.8338e-01,  2.4000e-01],\n",
       "          [-1.7417e-01, -6.2938e-02,  3.4574e-01,  ..., -1.0403e-02,\n",
       "           -1.8168e-01,  2.4901e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.4809e-02,  6.1465e-01,  3.7219e-01,  ...,  1.3810e-01,\n",
       "            2.8221e-01,  3.3853e-01],\n",
       "          [ 7.4193e-02,  6.1232e-01,  3.6982e-01,  ...,  1.3757e-01,\n",
       "            2.7952e-01,  3.3790e-01],\n",
       "          [ 7.4307e-02,  6.1321e-01,  3.7179e-01,  ...,  1.3761e-01,\n",
       "            2.8157e-01,  3.3771e-01]],\n",
       "\n",
       "         [[-1.1574e-01, -2.8281e-03,  2.9502e-01,  ..., -8.1345e-03,\n",
       "           -1.0811e-01,  1.4721e-01],\n",
       "          [-1.1755e-01, -1.0134e-02,  2.8978e-01,  ..., -1.3000e-02,\n",
       "           -1.1515e-01,  1.5038e-01],\n",
       "          [-1.1642e-01, -1.2529e-04,  3.0008e-01,  ..., -5.9540e-03,\n",
       "           -1.0637e-01,  1.4803e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 4.0669e-02,  6.8537e-01,  3.7044e-01,  ...,  1.6287e-01,\n",
       "            1.2541e-01,  4.6856e-01],\n",
       "          [ 4.2027e-02,  6.8445e-01,  3.7027e-01,  ...,  1.6284e-01,\n",
       "            1.2891e-01,  4.6626e-01],\n",
       "          [ 4.1797e-02,  6.8092e-01,  3.6718e-01,  ...,  1.6231e-01,\n",
       "            1.2621e-01,  4.6484e-01]],\n",
       "\n",
       "         [[-1.5246e-01, -1.0723e-02,  3.7205e-01,  ...,  1.9408e-02,\n",
       "           -1.3318e-01,  2.0899e-01],\n",
       "          [-1.5136e-01, -9.2273e-03,  3.7182e-01,  ...,  1.8823e-02,\n",
       "           -1.3193e-01,  2.0667e-01],\n",
       "          [-1.5145e-01, -8.3253e-03,  3.7278e-01,  ...,  1.9494e-02,\n",
       "           -1.3097e-01,  2.0687e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.0191e-02,  6.8104e-01,  4.2550e-01,  ...,  1.7730e-01,\n",
       "            1.9516e-01,  4.4645e-01],\n",
       "          [ 6.8809e-02,  6.7888e-01,  4.2032e-01,  ...,  1.7548e-01,\n",
       "            1.9309e-01,  4.4460e-01],\n",
       "          [ 6.8010e-02,  6.7424e-01,  4.1964e-01,  ...,  1.7489e-01,\n",
       "            1.9015e-01,  4.4297e-01]],\n",
       "\n",
       "         [[-1.7398e-01, -2.8866e-02,  3.6823e-01,  ...,  2.6098e-02,\n",
       "           -1.3584e-01,  2.5830e-01],\n",
       "          [-1.7410e-01, -3.0214e-02,  3.6690e-01,  ...,  2.3515e-02,\n",
       "           -1.3753e-01,  2.5797e-01],\n",
       "          [-1.7361e-01, -2.7800e-02,  3.6901e-01,  ...,  2.5256e-02,\n",
       "           -1.3534e-01,  2.5708e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8869e-02,  6.7132e-01,  3.6224e-01,  ...,  1.4229e-01,\n",
       "            2.4168e-01,  3.9211e-01],\n",
       "          [ 5.9346e-02,  6.7013e-01,  3.5901e-01,  ...,  1.4270e-01,\n",
       "            2.3867e-01,  3.9263e-01],\n",
       "          [ 5.9473e-02,  6.7049e-01,  3.5950e-01,  ...,  1.4270e-01,\n",
       "            2.3955e-01,  3.9247e-01]],\n",
       "\n",
       "         [[-1.4498e-01, -1.4674e-02,  3.4284e-01,  ..., -4.3445e-02,\n",
       "           -1.4218e-01,  1.8149e-01],\n",
       "          [-1.4637e-01, -1.7122e-02,  3.4272e-01,  ..., -4.4267e-02,\n",
       "           -1.4474e-01,  1.8398e-01],\n",
       "          [-1.4540e-01, -1.6710e-02,  3.4134e-01,  ..., -4.4888e-02,\n",
       "           -1.4421e-01,  1.8217e-01]]]], grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 query_dim,\n",
    "                 key_dim,\n",
    "                 num_units,\n",
    "                 dropout_p=0.1,\n",
    "                 h=8,\n",
    "                 is_masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        if query_dim != key_dim:\n",
    "            raise ValueError(\"query_dim and key_dim must be the same\")\n",
    "        if num_units % h != 0:\n",
    "            raise ValueError(\"num_units must be dividable by h\")\n",
    "        if query_dim != num_units:\n",
    "            raise ValueError(\"to employ residual connection, the number of \"\n",
    "                             \"query_dim and num_units must be the same\")\n",
    "\n",
    "        self._num_units = num_units\n",
    "        self._h = h\n",
    "        self._key_dim = float(key_dim)**-0.5\n",
    "        self._dropout_p = dropout_p\n",
    "        self._is_masked = is_masked\n",
    "\n",
    "        self.query_layer = nn.Linear(query_dim, num_units, bias=False)\n",
    "        self.key_layer = nn.Linear(key_dim, num_units, bias=False)\n",
    "        self.value_layer = nn.Linear(key_dim, num_units, bias=False)\n",
    "        self.proj_layer = nn.Linear(num_units, num_units)\n",
    "        self.ln = nn.LayerNorm(query_dim)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (torch.Tensor): [seq_len, batch, embed_dim]\n",
    "            keys (torch.Tensor): [seq_len, batch, embed_dim]\n",
    "        Returns:\n",
    "            torch.Tensor: [seq_len, batch, embed_dim]\n",
    "        \"\"\"\n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(keys)\n",
    "        V = self.value_layer(keys)\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        # split each Q, K and V into h different values from dim 2\n",
    "        # and then merge them back together in dim 0\n",
    "        chunk_size = int(self._num_units / self._h)\n",
    "        Q = Q.view(batch_size * self._h, seq_len, chunk_size)\n",
    "        K = K.view(batch_size * self._h, -1, chunk_size)\n",
    "        V = V.view(batch_size * self._h, -1, chunk_size)\n",
    "\n",
    "        # calculate QK^T\n",
    "        attention = torch.bmm(Q, K.transpose(1, 2))\n",
    "        # normalize with sqrt(dk)\n",
    "        attention = attention * self._key_dim\n",
    "        # use masking (usually for decoder) to prevent leftward\n",
    "        # information flow and retains auto-regressive property\n",
    "        # as said in the paper\n",
    "        if self._is_masked:\n",
    "            diag_vals = attention[0].sign().abs()\n",
    "            diag_mat = diag_vals.tril(abs(Q.size(1) - K.size(1)))\n",
    "            diag_mat = diag_mat.unsqueeze(0).expand(attention.size())\n",
    "            mask = torch.ones(\n",
    "                diag_mat.size(), device=query.device) * (-2**32 + 1)\n",
    "            # this is some trick that I use to combine the lower diagonal\n",
    "            # matrix and its masking. (diag_mat-1).abs() will reverse the value\n",
    "            # inside diag_mat, from 0 to 1 and 1 to zero. with this\n",
    "            # we don't need loop operation andn could perform our calculation\n",
    "            # faster\n",
    "            attention = (attention * diag_mat) + (mask * (diag_mat - 1).abs())\n",
    "        # put it to softmax\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        # apply dropout\n",
    "        attention = F.dropout(\n",
    "            attention, p=self._dropout_p, training=self.training)\n",
    "        # multiplyt it with V\n",
    "        attention = torch.bmm(attention, V)\n",
    "        # convert attention back to its input original size\n",
    "        attention = attention.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # apply  projection\n",
    "        attention = self.proj_layer(attention.view(-1, attention.size(-1)))\n",
    "        attention = attention.view(batch_size, seq_len, -1)\n",
    "        # residual connection\n",
    "        attention += query\n",
    "        # apply layer normalization\n",
    "        attention = self.ln(attention)\n",
    "\n",
    "        return attention\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.uniform_(self.query_layer.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.key_layer.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.value_layer.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.proj_layer.weight, -0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 17, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5385033d0c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "mh = MultiHeadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
